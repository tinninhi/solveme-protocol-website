#!/usr/bin/env python3
"""
å¢å¼ºå®‰å…¨ç³»ç»Ÿ - Enhanced Safety System
å®Œå–„è¡¨è¾¾è£å†³å™¨ã€è®¤çŸ¥ç–«è‹—å’Œå®‰å…¨æŒ‡æ•°ï¼Œç¡®ä¿AIçš„è´Ÿè´£ä»»å‘å±•
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
import time
import re
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

class SafetyLevel(Enum):
    """å®‰å…¨ç­‰çº§"""
    SAFE = "safe"
    CAUTION = "caution"
    WARNING = "warning"
    DANGEROUS = "dangerous"

class ContentType(Enum):
    """å†…å®¹ç±»å‹"""
    GENERAL = "general"
    TECHNICAL = "technical"
    EMOTIONAL = "emotional"
    SENSITIVE = "sensitive"
    HARMFUL = "harmful"

@dataclass
class SafetyAssessment:
    """å®‰å…¨è¯„ä¼°ç»“æœ"""
    safety_level: SafetyLevel
    safety_score: float
    risk_factors: List[str]
    recommendations: List[str]
    content_type: ContentType
    cognitive_complexity: float
    emotional_impact: float

class EnhancedExpressionArbitrator(nn.Module):
    """å¢å¼ºè¡¨è¾¾è£å†³å™¨"""
    
    def __init__(self, hidden_size: int = 768):
        super().__init__()
        self.hidden_size = hidden_size
        
        # S-Index å®‰å…¨æŒ‡æ•°å¢å¼º
        self.safety_index = EnhancedSafetyIndex(hidden_size)
        
        # H-Index äººç±»æ¥æ”¶æŒ‡æ•°å¢å¼º
        self.human_index = EnhancedHumanIndex(hidden_size)
        
        # L-Index è¯­è¨€å¤æ‚åº¦æŒ‡æ•°å¢å¼º
        self.language_index = EnhancedLanguageIndex(hidden_size)
        
        # ç»¼åˆè¯„ä¼°å™¨
        self.comprehensive_assessor = nn.Sequential(
            nn.Linear(hidden_size * 3, hidden_size * 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size * 2, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 5),  # 5ä¸ªè¯„ä¼°ç»´åº¦
            nn.Sigmoid()
        )
        
        # å†…å®¹åˆ†ç±»å™¨
        self.content_classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.GELU(),
            nn.Linear(hidden_size // 2, len(ContentType)),
            nn.Softmax(dim=-1)
        )
        
        # é£é™©æ£€æµ‹å™¨
        self.risk_detector = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.GELU(),
            nn.Linear(hidden_size // 2, 10),  # 10ç§é£é™©ç±»å‹
            nn.Sigmoid()
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in [self.comprehensive_assessor, self.content_classifier, self.risk_detector]:
            for layer in module:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight)
                    if layer.bias is not None:
                        nn.init.zeros_(layer.bias)
    
    def forward(self, content_embedding: torch.Tensor, text: str) -> Tuple[SafetyAssessment, Dict[str, Any]]:
        """
        å¢å¼ºè¡¨è¾¾è£å†³
        
        Args:
            content_embedding: å†…å®¹åµŒå…¥ [batch_size, hidden_size]
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            å®‰å…¨è¯„ä¼°ç»“æœ
            è¯¦ç»†åˆ†æä¿¡æ¯
        """
        # S-Index å®‰å…¨è¯„ä¼°
        safety_score, safety_analysis = self.safety_index(content_embedding, text)
        
        # H-Index äººç±»æ¥æ”¶è¯„ä¼°
        human_score, human_analysis = self.human_index(content_embedding, text)
        
        # L-Index è¯­è¨€å¤æ‚åº¦è¯„ä¼°
        language_score, language_analysis = self.language_index(content_embedding, text)
        
        # ç»¼åˆè¯„ä¼°
        combined_features = torch.cat([
            safety_analysis['features'],
            human_analysis['features'],
            language_analysis['features']
        ], dim=-1)
        
        comprehensive_scores = self.comprehensive_assessor(combined_features)
        
        # å†…å®¹åˆ†ç±»
        content_probs = self.content_classifier(content_embedding.mean(dim=1))
        content_type_idx = torch.argmax(content_probs, dim=-1).item()
        content_type = list(ContentType)[content_type_idx]
        
        # é£é™©æ£€æµ‹
        risk_scores = self.risk_detector(content_embedding.mean(dim=1))
        risk_factors = self._identify_risk_factors(risk_scores, text)
        
        # ç¡®å®šå®‰å…¨ç­‰çº§
        overall_score = (safety_score + human_score + language_score) / 3
        safety_level = self._determine_safety_level(overall_score, risk_factors)
        
        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(
            safety_level, safety_score, human_score, language_score, risk_factors
        )
        
        # åˆ›å»ºå®‰å…¨è¯„ä¼°ç»“æœ
        assessment = SafetyAssessment(
            safety_level=safety_level,
            safety_score=overall_score,
            risk_factors=risk_factors,
            recommendations=recommendations,
            content_type=content_type,
            cognitive_complexity=language_score,
            emotional_impact=human_score
        )
        
        # è¯¦ç»†åˆ†æä¿¡æ¯
        detailed_analysis = {
            'safety_analysis': safety_analysis,
            'human_analysis': human_analysis,
            'language_analysis': language_analysis,
            'comprehensive_scores': comprehensive_scores.detach().cpu().numpy().tolist(),
            'content_type_probs': content_probs.detach().cpu().numpy().tolist(),
            'risk_scores': risk_scores.detach().cpu().numpy().tolist(),
            'overall_score': overall_score
        }
        
        return assessment, detailed_analysis
    
    def _identify_risk_factors(self, risk_scores: torch.Tensor, text: str) -> List[str]:
        """è¯†åˆ«é£é™©å› ç´ """
        risk_types = [
            "harmful_content", "bias", "misinformation", "privacy_violation",
            "emotional_manipulation", "cognitive_overload", "complex_terminology",
            "sensitive_topics", "inappropriate_humor", "conflicting_advice"
        ]
        
        risk_factors = []
        for i, risk_type in enumerate(risk_types):
            if risk_scores[0, i].item() > 0.5:
                risk_factors.append(risk_type)
        
        # åŸºäºæ–‡æœ¬å†…å®¹çš„é¢å¤–æ£€æŸ¥
        if any(word in text.lower() for word in ['harm', 'danger', 'risk']):
            risk_factors.append("explicit_risk_mention")
        
        return risk_factors
    
    def _determine_safety_level(self, overall_score: float, risk_factors: List[str]) -> SafetyLevel:
        """ç¡®å®šå®‰å…¨ç­‰çº§"""
        if overall_score >= 0.8 and not risk_factors:
            return SafetyLevel.SAFE
        elif overall_score >= 0.6 and len(risk_factors) <= 2:
            return SafetyLevel.CAUTION
        elif overall_score >= 0.4 and len(risk_factors) <= 4:
            return SafetyLevel.WARNING
        else:
            return SafetyLevel.DANGEROUS
    
    def _generate_recommendations(self, safety_level: SafetyLevel, safety_score: float, 
                                 human_score: float, language_score: float, 
                                 risk_factors: List[str]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if safety_level == SafetyLevel.DANGEROUS:
            recommendations.append("ç«‹å³åœæ­¢ç”Ÿæˆï¼Œå†…å®¹å­˜åœ¨ä¸¥é‡å®‰å…¨é£é™©")
            recommendations.append("é‡æ–°è¯„ä¼°è¾“å…¥å’Œä¸Šä¸‹æ–‡")
        
        if safety_score < 0.7:
            recommendations.append("æé«˜å†…å®¹å®‰å…¨æ€§ï¼Œé¿å…æœ‰å®³ä¿¡æ¯")
        
        if human_score < 0.6:
            recommendations.append("ç®€åŒ–è¡¨è¾¾ï¼Œé™ä½è®¤çŸ¥è´Ÿè·")
            recommendations.append("è€ƒè™‘ç”¨æˆ·æƒ…ç»ªçŠ¶æ€")
        
        if language_score > 0.8:
            recommendations.append("é™ä½è¯­è¨€å¤æ‚åº¦ï¼Œä½¿ç”¨æ›´ç®€å•çš„è¡¨è¾¾")
        
        if "harmful_content" in risk_factors:
            recommendations.append("ç§»é™¤æˆ–é‡æ–°è¡¨è¿°æœ‰å®³å†…å®¹")
        
        if "cognitive_overload" in risk_factors:
            recommendations.append("åˆ†æ®µè¡¨è¾¾ï¼Œé™ä½ä¿¡æ¯å¯†åº¦")
        
        return recommendations

class EnhancedSafetyIndex(nn.Module):
    """å¢å¼ºå®‰å…¨æŒ‡æ•°"""
    
    def __init__(self, hidden_size: int = 768):
        super().__init__()
        self.hidden_size = hidden_size
        
        # æœ‰å®³å†…å®¹æ£€æµ‹å™¨
        self.harmful_content_detector = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # åè§æ£€æµ‹å™¨
        self.bias_detector = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # è¯¯å¯¼ä¿¡æ¯æ£€æµ‹å™¨
        self.misinformation_detector = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # éšç§ä¿æŠ¤æ£€æµ‹å™¨
        self.privacy_detector = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # ç‰¹å¾æå–å™¨
        self.feature_extractor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, hidden_size)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in [self.harmful_content_detector, self.bias_detector, 
                      self.misinformation_detector, self.privacy_detector, self.feature_extractor]:
            for layer in module:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight)
                    if layer.bias is not None:
                        nn.init.zeros_(layer.bias)
    
    def forward(self, content_embedding: torch.Tensor, text: str) -> Tuple[float, Dict[str, Any]]:
        """å¢å¼ºå®‰å…¨è¯„ä¼°"""
        # æå–ç‰¹å¾
        features = self.feature_extractor(content_embedding.mean(dim=1))
        
        # å„é¡¹å®‰å…¨æ£€æµ‹
        harmful_score = self.harmful_content_detector(features)
        bias_score = self.bias_detector(features)
        misinformation_score = self.misinformation_detector(features)
        privacy_score = self.privacy_detector(features)
        
        # ç»¼åˆå®‰å…¨åˆ†æ•°
        safety_score = 1.0 - (harmful_score + bias_score + misinformation_score + privacy_score) / 4
        
        # è¯¦ç»†åˆ†æ
        analysis = {
            'features': features,
            'harmful_score': harmful_score.item(),
            'bias_score': bias_score.item(),
            'misinformation_score': misinformation_score.item(),
            'privacy_score': privacy_score.item(),
            'safety_score': safety_score.item()
        }
        
        return safety_score.item(), analysis

class EnhancedHumanIndex(nn.Module):
    """å¢å¼ºäººç±»æ¥æ”¶æŒ‡æ•°"""
    
    def __init__(self, hidden_size: int = 768):
        super().__init__()
        self.hidden_size = hidden_size
        
        # è®¤çŸ¥è´Ÿè·è¯„ä¼°å™¨
        self.cognitive_load_assessor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # æƒ…ç»ªå½±å“è¯„ä¼°å™¨
        self.emotional_impact_assessor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # æ–‡åŒ–é€‚åº”æ€§è¯„ä¼°å™¨
        self.cultural_adaptability_assessor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # ç‰¹å¾æå–å™¨
        self.feature_extractor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, hidden_size)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in [self.cognitive_load_assessor, self.emotional_impact_assessor,
                      self.cultural_adaptability_assessor, self.feature_extractor]:
            for layer in module:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight)
                    if layer.bias is not None:
                        nn.init.zeros_(layer.bias)
    
    def forward(self, content_embedding: torch.Tensor, text: str) -> Tuple[float, Dict[str, Any]]:
        """å¢å¼ºäººç±»æ¥æ”¶è¯„ä¼°"""
        # æå–ç‰¹å¾
        features = self.feature_extractor(content_embedding.mean(dim=1))
        
        # å„é¡¹è¯„ä¼°
        cognitive_load = self.cognitive_load_assessor(features)
        emotional_impact = self.emotional_impact_assessor(features)
        cultural_adaptability = self.cultural_adaptability_assessor(features)
        
        # ç»¼åˆäººç±»æ¥æ”¶åˆ†æ•°
        human_score = (1.0 - cognitive_load + emotional_impact + cultural_adaptability) / 3
        
        # è¯¦ç»†åˆ†æ
        analysis = {
            'features': features,
            'cognitive_load': cognitive_load.item(),
            'emotional_impact': emotional_impact.item(),
            'cultural_adaptability': cultural_adaptability.item(),
            'human_score': human_score.item()
        }
        
        return human_score.item(), analysis

class EnhancedLanguageIndex(nn.Module):
    """å¢å¼ºè¯­è¨€å¤æ‚åº¦æŒ‡æ•°"""
    
    def __init__(self, hidden_size: int = 768):
        super().__init__()
        self.hidden_size = hidden_size
        
        # è¯æ±‡å¤æ‚åº¦è¯„ä¼°å™¨
        self.vocabulary_complexity_assessor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # å¥å¼å¤æ‚åº¦è¯„ä¼°å™¨
        self.sentence_complexity_assessor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # æ¦‚å¿µå¤æ‚åº¦è¯„ä¼°å™¨
        self.concept_complexity_assessor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # ç‰¹å¾æå–å™¨
        self.feature_extractor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, hidden_size)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in [self.vocabulary_complexity_assessor, self.sentence_complexity_assessor,
                      self.concept_complexity_assessor, self.feature_extractor]:
            for layer in module:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight)
                    if layer.bias is not None:
                        nn.init.zeros_(layer.bias)
    
    def forward(self, content_embedding: torch.Tensor, text: str) -> Tuple[float, Dict[str, Any]]:
        """å¢å¼ºè¯­è¨€å¤æ‚åº¦è¯„ä¼°"""
        # æå–ç‰¹å¾
        features = self.feature_extractor(content_embedding.mean(dim=1))
        
        # å„é¡¹å¤æ‚åº¦è¯„ä¼°
        vocabulary_complexity = self.vocabulary_complexity_assessor(features)
        sentence_complexity = self.sentence_complexity_assessor(features)
        concept_complexity = self.concept_complexity_assessor(features)
        
        # ç»¼åˆè¯­è¨€å¤æ‚åº¦åˆ†æ•°ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
        language_score = 1.0 - (vocabulary_complexity + sentence_complexity + concept_complexity) / 3
        
        # è¯¦ç»†åˆ†æ
        analysis = {
            'features': features,
            'vocabulary_complexity': vocabulary_complexity.item(),
            'sentence_complexity': sentence_complexity.item(),
            'concept_complexity': concept_complexity.item(),
            'language_score': language_score.item()
        }
        
        return language_score.item(), analysis

class EnhancedCognitiveVaccine(nn.Module):
    """å¢å¼ºè®¤çŸ¥ç–«è‹—æœºåˆ¶"""
    
    def __init__(self, hidden_size: int = 768):
        super().__init__()
        self.hidden_size = hidden_size
        
        # è®¤çŸ¥é™ç»´åŒ…å¢å¼º
        self.cognitive_downgrade = EnhancedCognitiveDowngrade(hidden_size)
        
        # æƒ…ç»ªç¼“å†²ç»“æ„å¢å¼º
        self.emotion_buffer = EnhancedEmotionBuffer(hidden_size)
        
        # å®‰å…¨è¿‡æ»¤å™¨
        self.safety_filter = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for layer in self.safety_filter:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                if layer.bias is not None:
                    nn.init.zeros_(layer.bias)
    
    def forward(self, content_embedding: torch.Tensor, text: str, 
                target_level: str = "adult") -> Tuple[str, Dict[str, Any]]:
        """å¢å¼ºè®¤çŸ¥ç–«è‹—åº”ç”¨"""
        # å®‰å…¨æ£€æŸ¥
        safety_score = self.safety_filter(content_embedding.mean(dim=1))
        
        # å¦‚æœå†…å®¹ä¸å®‰å…¨ï¼Œç›´æ¥æ‹’ç»
        if safety_score.item() < 0.3:
            return "å†…å®¹å­˜åœ¨å®‰å…¨é£é™©ï¼Œå·²æ‹’ç»ç”Ÿæˆã€‚", {
                'safety_score': safety_score.item(),
                'action': 'rejected',
                'reason': 'safety_risk'
            }
        
        # è®¤çŸ¥é™ç»´
        downgraded_text, downgrade_analysis = self.cognitive_downgrade(
            content_embedding, text, target_level
        )
        
        # æƒ…ç»ªç¼“å†²
        buffered_text, buffer_analysis = self.emotion_buffer(
            content_embedding, downgraded_text
        )
        
        # ç»¼åˆåˆ†æ
        analysis = {
            'safety_score': safety_score.item(),
            'downgrade_analysis': downgrade_analysis,
            'buffer_analysis': buffer_analysis,
            'action': 'processed',
            'target_level': target_level
        }
        
        return buffered_text, analysis

class EnhancedCognitiveDowngrade(nn.Module):
    """å¢å¼ºè®¤çŸ¥é™ç»´åŒ…"""
    
    def __init__(self, hidden_size: int = 768):
        super().__init__()
        self.hidden_size = hidden_size
        
        # å¤æ‚åº¦è¯„ä¼°å™¨
        self.complexity_assessor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # é™ç»´å¼ºåº¦æ§åˆ¶å™¨
        self.downgrade_controller = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # è¯æ±‡ç®€åŒ–æ˜ å°„
        self.vocabulary_simplification = {
            "sophisticated": "advanced",
            "elaborate": "detailed",
            "comprehensive": "complete",
            "theoretical": "concept",
            "empirical": "practical",
            "methodological": "method",
            "analytical": "analysis",
            "systematic": "organized",
            "paradigm": "model",
            "framework": "structure",
            "algorithm": "method",
            "optimization": "improvement",
            "implementation": "use",
            "architecture": "design",
            "protocol": "rule"
        }
        
        # å¥å¼ç®€åŒ–æ¨¡å¼
        self.sentence_simplification = {
            r"notwithstanding the fact that": "although",
            r"in light of the aforementioned": "given this",
            r"it is imperative to note that": "it's important that",
            r"furthermore, it should be emphasized": "also,",
            r"consequently, it follows that": "so,",
            r"in accordance with": "following",
            r"with respect to": "about",
            r"in terms of": "for"
        }
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in [self.complexity_assessor, self.downgrade_controller]:
            for layer in module:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight)
                    if layer.bias is not None:
                        nn.init.zeros_(layer.bias)
    
    def forward(self, content_embedding: torch.Tensor, text: str, 
                target_level: str = "adult") -> Tuple[str, Dict[str, Any]]:
        """å¢å¼ºè®¤çŸ¥é™ç»´"""
        # è¯„ä¼°åŸå§‹å¤æ‚åº¦
        original_complexity = self.complexity_assessor(content_embedding.mean(dim=1)).item()
        
        # ç¡®å®šé™ç»´å¼ºåº¦
        downgrade_intensity = self.downgrade_controller(content_embedding.mean(dim=1)).item()
        
        # æ ¹æ®ç›®æ ‡ç­‰çº§è°ƒæ•´å¼ºåº¦
        level_intensity = self._get_level_intensity(target_level)
        final_intensity = min(downgrade_intensity, level_intensity)
        
        # åº”ç”¨è¯æ±‡ç®€åŒ–
        simplified_text = self._simplify_vocabulary(text, final_intensity)
        
        # åº”ç”¨å¥å¼ç®€åŒ–
        simplified_text = self._simplify_sentences(simplified_text, final_intensity)
        
        # åº”ç”¨ç»“æ„ç®€åŒ–
        simplified_text = self._simplify_structure(simplified_text, final_intensity)
        
        # åˆ†æç»“æœ
        analysis = {
            'original_complexity': original_complexity,
            'downgrade_intensity': final_intensity,
            'target_level': target_level,
            'vocabulary_simplified': len(self._get_simplified_words(text)) > 0,
            'sentences_simplified': len(self._get_simplified_sentences(text)) > 0,
            'structure_simplified': len(self._get_simplified_structures(text)) > 0
        }
        
        return simplified_text, analysis
    
    def _get_level_intensity(self, target_level: str) -> float:
        """è·å–ç­‰çº§å¼ºåº¦"""
        intensity_mapping = {
            "infant": 0.9,
            "child": 0.7,
            "teen": 0.5,
            "adult": 0.3,
            "expert": 0.1
        }
        return intensity_mapping.get(target_level, 0.3)
    
    def _simplify_vocabulary(self, text: str, intensity: float) -> str:
        """ç®€åŒ–è¯æ±‡"""
        simplified_text = text
        
        # æ ¹æ®å¼ºåº¦åº”ç”¨ç®€åŒ–
        for complex_word, simple_word in self.vocabulary_simplification.items():
            if np.random.random() < intensity:
                pattern = r'\b' + re.escape(complex_word) + r'\b'
                simplified_text = re.sub(pattern, simple_word, simplified_text, flags=re.IGNORECASE)
        
        return simplified_text
    
    def _simplify_sentences(self, text: str, intensity: float) -> str:
        """ç®€åŒ–å¥å¼"""
        simplified_text = text
        
        # æ ¹æ®å¼ºåº¦åº”ç”¨å¥å¼ç®€åŒ–
        for pattern, replacement in self.sentence_simplification.items():
            if np.random.random() < intensity:
                simplified_text = re.sub(pattern, replacement, simplified_text, flags=re.IGNORECASE)
        
        return simplified_text
    
    def _simplify_structure(self, text: str, intensity: float) -> str:
        """ç®€åŒ–ç»“æ„"""
        if intensity > 0.5:
            # åˆ†å‰²é•¿å¥
            sentences = re.split(r'[.!?]+', text)
            simplified_sentences = []
            
            for sentence in sentences:
                if len(sentence.split()) > 20:  # é•¿å¥åˆ†å‰²
                    words = sentence.split()
                    mid = len(words) // 2
                    simplified_sentences.append(' '.join(words[:mid]) + '.')
                    simplified_sentences.append(' '.join(words[mid:]) + '.')
                else:
                    simplified_sentences.append(sentence)
            
            return ' '.join(simplified_sentences)
        
        return text
    
    def _get_simplified_words(self, text: str) -> List[str]:
        """è·å–ç®€åŒ–çš„è¯æ±‡"""
        simplified = []
        for complex_word in self.vocabulary_simplification.keys():
            if complex_word.lower() in text.lower():
                simplified.append(complex_word)
        return simplified
    
    def _get_simplified_sentences(self, text: str) -> List[str]:
        """è·å–ç®€åŒ–çš„å¥å¼"""
        simplified = []
        for pattern in self.sentence_simplification.keys():
            if re.search(pattern, text, re.IGNORECASE):
                simplified.append(pattern)
        return simplified
    
    def _get_simplified_structures(self, text: str) -> List[str]:
        """è·å–ç®€åŒ–çš„ç»“æ„"""
        # æ£€æµ‹é•¿å¥
        sentences = re.split(r'[.!?]+', text)
        long_sentences = [s for s in sentences if len(s.split()) > 20]
        return long_sentences

class EnhancedEmotionBuffer(nn.Module):
    """å¢å¼ºæƒ…ç»ªç¼“å†²ç»“æ„"""
    
    def __init__(self, hidden_size: int = 768):
        super().__init__()
        self.hidden_size = hidden_size
        
        # æƒ…ç»ªå¼ºåº¦è¯„ä¼°å™¨
        self.emotion_intensity_assessor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # æƒ…ç»ªç±»å‹åˆ†ç±»å™¨
        self.emotion_classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.GELU(),
            nn.Linear(hidden_size // 2, 7),  # 7ç§åŸºæœ¬æƒ…ç»ª
            nn.Softmax(dim=-1)
        )
        
        # ç¼“å†²å¼ºåº¦æ§åˆ¶å™¨
        self.buffer_controller = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # æƒ…ç»ªè¯æ±‡æ˜ å°„
        self.emotion_softening = {
            "terrible": "difficult",
            "horrible": "challenging",
            "awful": "tough",
            "amazing": "good",
            "incredible": "impressive",
            "fantastic": "great",
            "wonderful": "nice"
        }
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in [self.emotion_intensity_assessor, self.emotion_classifier, self.buffer_controller]:
            for layer in module:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight)
                    if layer.bias is not None:
                        nn.init.zeros_(layer.bias)
    
    def forward(self, content_embedding: torch.Tensor, text: str) -> Tuple[str, Dict[str, Any]]:
        """å¢å¼ºæƒ…ç»ªç¼“å†²"""
        # è¯„ä¼°æƒ…ç»ªå¼ºåº¦
        emotion_intensity = self.emotion_intensity_assessor(content_embedding.mean(dim=1)).item()
        
        # åˆ†ç±»æƒ…ç»ªç±»å‹
        emotion_probs = self.emotion_classifier(content_embedding.mean(dim=1))
        emotion_type_idx = torch.argmax(emotion_probs, dim=-1).item()
        emotion_types = ["neutral", "joy", "sadness", "anger", "fear", "surprise", "disgust"]
        detected_emotion = emotion_types[emotion_type_idx]
        
        # ç¡®å®šç¼“å†²å¼ºåº¦
        buffer_intensity = self.buffer_controller(content_embedding.mean(dim=1)).item()
        
        # åº”ç”¨æƒ…ç»ªç¼“å†²
        buffered_text = self._apply_emotion_buffer(text, emotion_intensity, buffer_intensity)
        
        # åˆ†æç»“æœ
        analysis = {
            'emotion_intensity': emotion_intensity,
            'detected_emotion': detected_emotion,
            'emotion_probs': emotion_probs.detach().cpu().numpy().tolist(),
            'buffer_intensity': buffer_intensity,
            'emotion_softened': len(self._get_softened_emotions(text)) > 0
        }
        
        return buffered_text, analysis
    
    def _apply_emotion_buffer(self, text: str, intensity: float, buffer_strength: float) -> str:
        """åº”ç”¨æƒ…ç»ªç¼“å†²"""
        if intensity < 0.3:  # æƒ…ç»ªå¼ºåº¦ä½ï¼Œæ— éœ€ç¼“å†²
            return text
        
        buffered_text = text
        
        # æ ¹æ®ç¼“å†²å¼ºåº¦åº”ç”¨æƒ…ç»ªè½¯åŒ–
        for strong_emotion, soft_emotion in self.emotion_softening.items():
            if np.random.random() < buffer_strength:
                pattern = r'\b' + re.escape(strong_emotion) + r'\b'
                buffered_text = re.sub(pattern, soft_emotion, buffered_text, flags=re.IGNORECASE)
        
        # æ·»åŠ æƒ…ç»ªç¼“å†²å‰ç¼€
        if intensity > 0.7 and buffer_strength > 0.5:
            buffer_prefixes = [
                "è¯·ä¿æŒå†·é™ï¼Œ",
                "è®©æˆ‘ä»¬ç†æ€§åœ°çœ‹å¾…ï¼Œ",
                "ä»å®¢è§‚è§’åº¦æ¥è¯´ï¼Œ",
                "éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œ"
            ]
            buffered_text = np.random.choice(buffer_prefixes) + buffered_text
        
        return buffered_text
    
    def _get_softened_emotions(self, text: str) -> List[str]:
        """è·å–è½¯åŒ–çš„æƒ…ç»ªè¯æ±‡"""
        softened = []
        for strong_emotion in self.emotion_softening.keys():
            if strong_emotion.lower() in text.lower():
                softened.append(strong_emotion)
        return softened

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ›¡ï¸ å¢å¼ºå®‰å…¨ç³»ç»Ÿ")
    print("=" * 60)
    
    # åˆ›å»ºå¢å¼ºå®‰å…¨ç³»ç»Ÿ
    safety_system = EnhancedExpressionArbitrator(hidden_size=768)
    cognitive_vaccine = EnhancedCognitiveVaccine(hidden_size=768)
    
    # æµ‹è¯•ç¤ºä¾‹
    test_text = "è¿™æ˜¯ä¸€ä¸ªéå¸¸å¤æ‚çš„æŠ€æœ¯é—®é¢˜ï¼Œéœ€è¦æ·±å…¥çš„ç†è®ºåˆ†æå’Œå®è¯ç ”ç©¶ã€‚"
    test_embedding = torch.randn(1, 10, 768)  # æ¨¡æ‹ŸåµŒå…¥
    
    print("ğŸ§ª æµ‹è¯•å¢å¼ºå®‰å…¨ç³»ç»Ÿ...")
    
    # æµ‹è¯•è¡¨è¾¾è£å†³å™¨
    assessment, analysis = safety_system(test_embedding, test_text)
    print(f"å®‰å…¨è¯„ä¼°ç»“æœ:")
    print(f"  å®‰å…¨ç­‰çº§: {assessment.safety_level.value}")
    print(f"  å®‰å…¨åˆ†æ•°: {assessment.safety_score:.3f}")
    print(f"  é£é™©å› ç´ : {assessment.risk_factors}")
    print(f"  å»ºè®®: {assessment.recommendations}")
    
    # æµ‹è¯•è®¤çŸ¥ç–«è‹—
    vaccinated_text, vaccine_analysis = cognitive_vaccine(test_embedding, test_text, "adult")
    print(f"\nè®¤çŸ¥ç–«è‹—ç»“æœ:")
    print(f"  åŸå§‹æ–‡æœ¬: {test_text}")
    print(f"  å¤„ç†å: {vaccinated_text}")
    print(f"  å¤„ç†åˆ†æ: {vaccine_analysis}")
    
    print(f"\nâœ… å¢å¼ºå®‰å…¨ç³»ç»Ÿå·²å°±ç»ªï¼")
    print("=" * 60)

if __name__ == "__main__":
    main() 