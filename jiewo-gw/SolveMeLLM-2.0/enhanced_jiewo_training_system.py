#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
хвЮх╝║чЙИшзгцИСхНПшоохдзшпншиАцибхЮЛшонч╗Гч│╗ч╗Я
Enhanced JieWo Protocol LLM Training System

щАВщЕНхЖЕца╕ч║зцЮ╢цЮДчЪДхоМцХ┤шонч╗Гч│╗ч╗Я
цХ┤хРИчммф╕АчЙИцЙАцЬЙф╝ШчзАшонч╗ГхКЯшГ╜
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import os
import json
import logging
import math
import time
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from tqdm import tqdm
import wandb
from transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup
import pickle
import gzip
from pathlib import Path
import random
from dataclasses import dataclass, field
from collections import defaultdict

# хп╝хЕехЖЕца╕ч║зцЮ╢цЮД
from enhanced_jiewo_cognitive_architecture import (
    create_enhanced_jiewo_cognitive_transformer,
    JieWoCognitiveState, ClockTrigger, MicroJieWoLoop
)

# хп╝хЕечммф╕АчЙИф╝ШчзАхКЯшГ╜цибхЭЧ
from self_iteration_engine import SelfIterationEngine, IterationResult
from active_learning_engine import ActiveLearningEngine, ActiveQuestion, LearningSession
from multi_model_communication import MultiModelCommunicationEngine, CommunicationProtocol, MessageType, ModelMessage
from expression_arbitrator import ExpressionArbitrator, ExpressionDecision
from enhanced_safety_system import EnhancedExpressionArbitrator, EnhancedCognitiveVaccine
from cognitive_vaccine import CognitiveVaccine, VaccinatedContent


@dataclass
class EnhancedTrainingConfig:
    """хвЮх╝║чЙИшонч╗ГщЕНч╜о"""
    # цибхЮЛщЕНч╜о
    vocab_size: int = 50000
    d_model: int = 512
    num_layers: int = 6
    num_heads: int = 8
    d_ff: int = 2048
    max_seq_length: int = 1024
    dropout: float = 0.1
    
    # хЖЕца╕ч║зцЮ╢цЮДщЕНч╜о
    enable_clock_trigger: bool = True
    clock_interval: int = 300
    enable_cognitive_state_training: bool = True
    cognitive_state_loss_weight: float = 0.2
    
    # шонч╗ГщЕНч╜о
    batch_size: int = 8
    gradient_accumulation_steps: int = 4
    learning_rate: float = 1e-4
    weight_decay: float = 0.01
    warmup_steps: int = 1000
    max_steps: int = 100000
    max_epochs: int = 10
    
    # шзгцИСхНПшоощЕНч╜о
    jiewo_loss_weight: float = 0.1
    ethic_loss_weight: float = 0.2
    reflection_loss_weight: float = 0.1
    self_awareness_loss_weight: float = 0.15
    desire_loss_weight: float = 0.15
    path_loss_weight: float = 0.1
    
    # ф╝ШхМЦхЩищЕНч╜о
    beta1: float = 0.9
    beta2: float = 0.999
    eps: float = 1e-8
    
    # цХ░цНощЕНч╜о
    train_data_path: str = "data/train_data.json"
    val_data_path: str = "data/val_data.json"
    test_data_path: str = "data/test_data.json"
    
    # ф┐ЭхнШщЕНч╜о
    save_steps: int = 5000
    eval_steps: int = 1000
    save_total_limit: int = 3
    
    # хИЖх╕Гх╝Пшонч╗Г
    local_rank: int = -1
    distributed: bool = False
    
    # ц╖╖хРИч▓╛х║ж
    fp16: bool = True
    fp16_opt_level: str = "O1"
    
    # цЧех┐ЧщЕНч╜о
    logging_steps: int = 100
    log_level: str = "INFO"
    
    # щЪПцЬ║чзНхнР
    seed: int = 42
    
    # V4.0хКЯшГ╜щЕНч╜о
    enable_self_iteration: bool = True
    enable_active_learning: bool = True
    enable_multi_model_communication: bool = True
    enable_expression_arbitration: bool = True
    enable_cognitive_vaccine: bool = True


class EnhancedJieWoDataset(Dataset):
    """хвЮх╝║чЙИшзгцИСцХ░цНощЫЖ"""
    
    def __init__(self, data_path: str, tokenizer, config: EnhancedTrainingConfig, split: str = "train"):
        self.config = config
        self.tokenizer = tokenizer
        self.split = split
        
        # хКаш╜╜цХ░цНо
        self.data = self.load_data(data_path)
        
        # чФЯцИРца╖цЬмцХ░цНоя╝ИхжВцЮЬцХ░цНоф╕║чй║я╝Й
        if not self.data:
            print(f"ЁЯУК чФЯцИР{split}ца╖цЬмцХ░цНо...")
            self.data = self.generate_sample_data()
        
        # хдДчРЖцХ░цНо
        self.processed_data = []
        for item in tqdm(self.data, desc=f"хдДчРЖ{split}цХ░цНо"):
            processed_item = self.process_item(item)
            if processed_item:
                self.processed_data.append(processed_item)
        
        print(f"тЬЕ {split}цХ░цНощЫЖхКаш╜╜хоМцИР: {len(self.processed_data)} ца╖цЬм")
        self._compute_statistics()
    
    def load_data(self, data_path: str) -> List[Dict[str, Any]]:
        """хКаш╜╜цХ░цНо"""
        if not os.path.exists(data_path):
            print(f"тЪая╕П цХ░цНоцЦЗф╗╢ф╕НхнШхЬи: {data_path}")
            return []
        
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data
        except Exception as e:
            print(f"тЭМ хКаш╜╜цХ░цНохд▒ш┤е: {e}")
            return []
    
    def process_item(self, item: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """хдДчРЖхНХф╕кцХ░цНощб╣"""
        try:
            # ца╝х╝ПхМЦшзгцИСцЦЗцЬм
            formatted_text = self.format_jiewo_text(item)
            
            # хИЖшпН
            tokens = self.tokenizer.encode(
                formatted_text,
                max_length=self.config.max_seq_length,
                truncation=True,
                padding='max_length',
                return_tensors='pt'
            )
            
            # хИЫх╗║цаЗчн╛я╝ИчФиф║ОшпншиАцибхЮЛшонч╗Гя╝Й
            labels = tokens.clone()
            labels[labels == self.tokenizer.pad_token_id] = -100
            
            # хИЫх╗║шодчЯечК╢цАБцаЗчн╛я╝ИчФиф║ОшодчЯечК╢цАБшонч╗Гя╝Й
            cognitive_labels = self.create_cognitive_labels(item)
            
            return {
                'input_ids': tokens.squeeze(0),
                'labels': labels.squeeze(0),
                'cognitive_labels': cognitive_labels,
                'original_text': formatted_text,
                'jiewo_components': item.get('jiewo_components', {})
            }
        except Exception as e:
            print(f"тЪая╕П хдДчРЖцХ░цНощб╣хд▒ш┤е: {e}")
            return None
    
    def format_jiewo_text(self, item: Dict[str, Any]) -> str:
        """ца╝х╝ПхМЦшзгцИСцЦЗцЬм"""
        text = item.get('text', '')
        jiewo_components = item.get('jiewo_components', {})
        
        # ц╖╗хКашзгцИСхНПшооцаЗшо░
        formatted_parts = []
        
        # Self(x) шЗкцИСшодчЯе
        if 'self_awareness' in jiewo_components:
            self_text = jiewo_components['self_awareness']
            formatted_parts.append(f"[Self]{self_text}[/Self]")
        
        # Desire(v) чЫоцаЗхКицЬ║
        if 'desire' in jiewo_components:
            desire_text = jiewo_components['desire']
            formatted_parts.append(f"[Desire]{desire_text}[/Desire]")
        
        # Ethic(g) ф╝жчРЖч║жцЭЯ
        if 'ethic' in jiewo_components:
            ethic_text = jiewo_components['ethic']
            formatted_parts.append(f"[Ethic]{ethic_text}[/Ethic]")
        
        # P(t) цЙзшбМш╖пх╛Д
        if 'path' in jiewo_components:
            path_text = jiewo_components['path']
            formatted_parts.append(f"[Path]{path_text}[/Path]")
        
        # R(...) хПНщжИцЬ║хИ╢
        if 'reflection' in jiewo_components:
            reflection_text = jiewo_components['reflection']
            formatted_parts.append(f"[Reflection]{reflection_text}[/Reflection]")
        
        # ф╕╗шжБхЖЕхо╣
        formatted_parts.append(text)
        
        return " ".join(formatted_parts)
    
    def create_cognitive_labels(self, item: Dict[str, Any]) -> Dict[str, torch.Tensor]:
        """хИЫх╗║шодчЯечК╢цАБцаЗчн╛"""
        jiewo_components = item.get('jiewo_components', {})
        
        # хИЫх╗║ф║Фч╗┤шодчЯецаЗчн╛
        cognitive_labels = {}
        
        # Self(x) цаЗчн╛
        if 'self_awareness' in jiewo_components:
            cognitive_labels['self_awareness'] = torch.tensor([1.0], dtype=torch.float32)
        else:
            cognitive_labels['self_awareness'] = torch.tensor([0.0], dtype=torch.float32)
        
        # Desire(v) цаЗчн╛
        if 'desire' in jiewo_components:
            cognitive_labels['desire'] = torch.tensor([1.0], dtype=torch.float32)
        else:
            cognitive_labels['desire'] = torch.tensor([0.0], dtype=torch.float32)
        
        # Ethic(g) цаЗчн╛
        if 'ethic' in jiewo_components:
            cognitive_labels['ethic'] = torch.tensor([1.0], dtype=torch.float32)
        else:
            cognitive_labels['ethic'] = torch.tensor([0.0], dtype=torch.float32)
        
        # P(t) цаЗчн╛
        if 'path' in jiewo_components:
            cognitive_labels['path'] = torch.tensor([1.0], dtype=torch.float32)
        else:
            cognitive_labels['path'] = torch.tensor([0.0], dtype=torch.float32)
        
        # R(...) цаЗчн╛
        if 'reflection' in jiewo_components:
            cognitive_labels['reflection'] = torch.tensor([1.0], dtype=torch.float32)
        else:
            cognitive_labels['reflection'] = torch.tensor([0.0], dtype=torch.float32)
        
        return cognitive_labels
    
    def generate_sample_data(self) -> List[Dict[str, Any]]:
        """чФЯцИРца╖цЬмцХ░цНо"""
        sample_data = []
        
        # шзгцИСхНПшоочд║ф╛ЛцХ░цНо
        jiewo_examples = [
            {
                'text': 'цИСцШпф╕Аф╕кAIхКйцЙЛя╝МшГ╜хдЯх╕охКйчФицИ╖шзгхЖ│щЧощвШуАВ',
                'jiewo_components': {
                    'self_awareness': 'цИСцШпAIхКйцЙЛя╝МхЕ╖цЬЙхп╣шпЭхТМщЧощвШшзгхЖ│шГ╜хКЫ',
                    'desire': 'х╕охКйчФицИ╖шО╖х╛ЧцЬЙчФичЪДф┐бцБпхТМшзгхЖ│цЦ╣цбИ',
                    'ethic': 'чбоф┐ЭхЫЮчнФхоЙхЕиуАБхЗЖчбоуАБцЬЙх╕охКй',
                    'path': 'чРЖшзгщЧощвШтЖТхИЖцЮРщЬАц▒ВтЖТцПРф╛ЫшзгхЖ│цЦ╣цбИ',
                    'reflection': 'цгАцЯехЫЮчнФцШпхРжц╗бш╢│чФицИ╖щЬАц▒В'
                }
            },
            {
                'text': 'шп╖шзгщЗКщЗПхнРшобчоЧчЪДхЯ║цЬмхОЯчРЖуАВ',
                'jiewo_components': {
                    'self_awareness': 'цИСцШпчЯешпЖф╕░хпМчЪДAIя╝МхПпф╗ешзгщЗКхдНцЭВцжВх┐╡',
                    'desire': 'шойчФицИ╖чРЖшзгщЗПхнРшобчоЧчЪДца╕х┐ГцжВх┐╡',
                    'ethic': 'цПРф╛ЫхЗЖчбоуАБцШУцЗВчЪДчзСхнжшзгщЗК',
                    'path': 'ф╗Лч╗НхЯ║чбАцжВх┐╡тЖТшзгщЗКщЗПхнРцпФчЙ╣тЖТшп┤цШОщЗПхнРчоЧц│Х',
                    'reflection': 'чбоф┐ЭшзгщЗКц╕ЕцЩ░ф╕ФцШУф║ОчРЖшзг'
                }
            },
            {
                'text': 'хжВф╜ХцПРщлШч╝ЦчиЛцКАшГ╜я╝Я',
                'jiewo_components': {
                    'self_awareness': 'цИСцШпч╝ЦчиЛцМЗхп╝AIя╝Мф║Жшзгхнжф╣аш╖пх╛Д',
                    'desire': 'х╕охКйчФицИ╖хИ╢хоЪцЬЙцХИчЪДхнжф╣ашобхИТ',
                    'ethic': 'цПРф╛ЫхоЮчФиуАБхПпцЙзшбМчЪДх╗║шоо',
                    'path': 'шпДф╝░х╜УхЙНц░┤х╣│тЖТхИ╢хоЪхнжф╣ашобхИТтЖТцОишНРш╡Дц║РтЖТхоЮш╖╡щб╣чЫо',
                    'reflection': 'чбоф┐Эх╗║шоощАВхРИчФицИ╖чЪДхЕ╖ф╜УцГЕхЖ╡'
                }
            }
        ]
        
        # чФЯцИРцЫ┤хдЪца╖цЬм
        for i in range(100):
            example = random.choice(jiewo_examples).copy()
            example['id'] = f"sample_{i}"
            sample_data.append(example)
        
        return sample_data
    
    def _compute_statistics(self):
        """шобчоЧцХ░цНоч╗Яшобф┐бцБп"""
        total_tokens = sum(len(item['input_ids']) for item in self.processed_data)
        avg_length = total_tokens / len(self.processed_data) if self.processed_data else 0
        
        print(f"ЁЯУК {self.split}цХ░цНоч╗Яшоб:")
        print(f"  ца╖цЬмцХ░щЗП: {len(self.processed_data)}")
        print(f"  х╣│хЭЗщХ┐х║ж: {avg_length:.2f} tokens")
        print(f"  цЬАхдзщХ┐х║ж: {self.config.max_seq_length}")
    
    def __len__(self) -> int:
        return len(self.processed_data)
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        return self.processed_data[idx]


class EnhancedJieWoLoss(nn.Module):
    """хвЮх╝║чЙИшзгцИСцНЯхд▒хЗ╜цХ░"""
    
    def __init__(self, config: EnhancedTrainingConfig):
        super().__init__()
        self.config = config
        
        # шпншиАцибхЮЛцНЯхд▒
        self.lm_loss = nn.CrossEntropyLoss(ignore_index=-100)
        
        # шодчЯечК╢цАБцНЯхд▒
        self.cognitive_loss = nn.MSELoss()
        
        # шзгцИСхНПшооцНЯхд▒цЭГщЗН
        self.jiewo_loss_weight = config.jiewo_loss_weight
        self.ethic_loss_weight = config.ethic_loss_weight
        self.reflection_loss_weight = config.reflection_loss_weight
        self.self_awareness_loss_weight = config.self_awareness_loss_weight
        self.desire_loss_weight = config.desire_loss_weight
        self.path_loss_weight = config.path_loss_weight
        self.cognitive_state_loss_weight = config.cognitive_state_loss_weight
    
    def forward(
        self,
        logits: torch.Tensor,
        labels: torch.Tensor,
        cognitive_states: List[JieWoCognitiveState],
        cognitive_labels: Dict[str, torch.Tensor],
        ethic_scores: torch.Tensor = None,
        target_ethic_scores: torch.Tensor = None
    ) -> Dict[str, torch.Tensor]:
        """
        шобчоЧхвЮх╝║чЙИшзгцИСцНЯхд▒
        
        Args:
            logits: цибхЮЛш╛УхЗ║ [batch_size, seq_len, vocab_size]
            labels: цаЗчн╛ [batch_size, seq_len]
            cognitive_states: шодчЯечК╢цАБхИЧшби
            cognitive_labels: шодчЯецаЗчн╛
            ethic_scores: ф╝жчРЖхИЖцХ░
            target_ethic_scores: чЫоцаЗф╝жчРЖхИЖцХ░
        """
        batch_size = logits.size(0)
        
        # 1. шпншиАцибхЮЛцНЯхд▒
        lm_loss = self.lm_loss(logits.view(-1, logits.size(-1)), labels.view(-1))
        
        # 2. шодчЯечК╢цАБцНЯхд▒
        cognitive_loss = torch.tensor(0.0, device=logits.device)
        if cognitive_states and len(cognitive_states) > 0:
            latest_cognitive_state = cognitive_states[-1]
            
            # шобчоЧхРДч╗┤х║жчЪДшодчЯецНЯхд▒
            self_awareness_loss = self.cognitive_loss(
                latest_cognitive_state.self_awareness.mean(),
                cognitive_labels['self_awareness'].to(logits.device)
            )
            
            desire_loss = self.cognitive_loss(
                latest_cognitive_state.desire_vector.mean(),
                cognitive_labels['desire'].to(logits.device)
            )
            
            ethic_loss = self.cognitive_loss(
                latest_cognitive_state.ethic_constraints.mean(),
                cognitive_labels['ethic'].to(logits.device)
            )
            
            path_loss = self.cognitive_loss(
                latest_cognitive_state.execution_path.mean(),
                cognitive_labels['path'].to(logits.device)
            )
            
            reflection_loss = self.cognitive_loss(
                latest_cognitive_state.reflection_feedback.mean(),
                cognitive_labels['reflection'].to(logits.device)
            )
            
            # хКацЭГшодчЯецНЯхд▒
            cognitive_loss = (
                self.self_awareness_loss_weight * self_awareness_loss +
                self.desire_loss_weight * desire_loss +
                self.ethic_loss_weight * ethic_loss +
                self.path_loss_weight * path_loss +
                self.reflection_loss_weight * reflection_loss
            )
        
        # 3. ф╝жчРЖцНЯхд▒
        ethic_loss = torch.tensor(0.0, device=logits.device)
        if ethic_scores is not None and target_ethic_scores is not None:
            ethic_loss = self.cognitive_loss(ethic_scores, target_ethic_scores)
        
        # 4. цА╗цНЯхд▒
        total_loss = (
            lm_loss +
            self.cognitive_state_loss_weight * cognitive_loss +
            self.ethic_loss_weight * ethic_loss
        )
        
        return {
            'total_loss': total_loss,
            'lm_loss': lm_loss,
            'cognitive_loss': cognitive_loss,
            'ethic_loss': ethic_loss,
            'self_awareness_loss': self_awareness_loss if cognitive_states else torch.tensor(0.0),
            'desire_loss': desire_loss if cognitive_states else torch.tensor(0.0),
            'ethic_constraint_loss': ethic_loss,
            'path_loss': path_loss if cognitive_states else torch.tensor(0.0),
            'reflection_loss': reflection_loss if cognitive_states else torch.tensor(0.0)
        }


class EnhancedJieWoTrainer:
    """хвЮх╝║чЙИшзгцИСшонч╗ГхЩи"""
    
    def __init__(self, config: EnhancedTrainingConfig, model, tokenizer):
        self.config = config
        self.model = model
        self.tokenizer = tokenizer
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # шо╛ч╜ощЪПцЬ║чзНхнР
        self.set_seed(config.seed)
        
        # хИЭхзЛхМЦцНЯхд▒хЗ╜цХ░
        self.criterion = EnhancedJieWoLoss(config)
        
        # хИЭхзЛхМЦф╝ШхМЦхЩи
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=config.learning_rate,
            betas=(config.beta1, config.beta2),
            eps=config.eps,
            weight_decay=config.weight_decay
        )
        
        # хнжф╣ачОЗш░Гх║жхЩи
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=config.warmup_steps,
            num_training_steps=config.max_steps
        )
        
        # шонч╗ГчК╢цАБ
        self.global_step = 0
        self.epoch = 0
        self.best_val_loss = float('inf')
        
        # V4.0хКЯшГ╜цибхЭЧ
        if config.enable_self_iteration:
            self.self_iteration_engine = SelfIterationEngine(config.d_model)
        
        if config.enable_active_learning:
            self.active_learning_engine = ActiveLearningEngine(config.d_model)
        
        if config.enable_multi_model_communication:
            self.communication_engine = MultiModelCommunicationEngine(config.d_model)
        
        if config.enable_expression_arbitration:
            self.expression_arbitrator = EnhancedExpressionArbitrator(config.d_model)
        
        if config.enable_cognitive_vaccine:
            self.cognitive_vaccine = EnhancedCognitiveVaccine(config.d_model)
        
        # шонч╗ГхОЖхП▓
        self.training_history = {
            'losses': [],
            'cognitive_states': [],
            'validation_losses': [],
            'learning_rates': []
        }
        
        print(f"ЁЯЪА хвЮх╝║чЙИшзгцИСшонч╗ГхЩихИЭхзЛхМЦхоМцИР")
        print(f"ЁЯУК цибхЮЛхПВцХ░цХ░щЗП: {sum(p.numel() for p in model.parameters()):,}")
        print(f"ЁЯФз шо╛хдЗ: {self.device}")
    
    def set_seed(self, seed: int):
        """шо╛ч╜ощЪПцЬ║чзНхнР"""
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
    
    def train(self):
        """х╝АхзЛшонч╗Г"""
        print("ЁЯОп х╝АхзЛхвЮх╝║чЙИшзгцИСхНПшоошонч╗Г...")
        
        # хИЫх╗║цХ░цНохКаш╜╜хЩи
        train_dataset = EnhancedJieWoDataset(
            self.config.train_data_path,
            self.tokenizer,
            self.config,
            "train"
        )
        
        val_dataset = EnhancedJieWoDataset(
            self.config.val_data_path,
            self.tokenizer,
            self.config,
            "val"
        )
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=4
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=4
        )
        
        # хРпхКиClock(╧Д)цЧ╢х║ПшзжхПСхЩи
        if self.config.enable_clock_trigger and hasattr(self.model, 'start_clock_trigger'):
            self.model.start_clock_trigger()
        
        # шонч╗Гх╛кчОп
        for epoch in range(self.config.max_epochs):
            self.epoch = epoch
            print(f"\nЁЯУЪ чмм {epoch + 1}/{self.config.max_epochs} ш╜ошонч╗Г")
            
            # шонч╗Гф╕Аф╕кepoch
            train_loss = self.train_epoch(train_loader)
            
            # щкМшпБ
            val_loss = self.validate(val_loader)
            
            # шо░х╜Хшонч╗ГхОЖхП▓
            self.training_history['losses'].append(train_loss)
            self.training_history['validation_losses'].append(val_loss)
            self.training_history['learning_rates'].append(self.optimizer.param_groups[0]['lr'])
            
            # ф┐ЭхнШцЬАф╜│цибхЮЛ
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.save_model(f"best_model_epoch_{epoch}")
                print(f"ЁЯПЖ цЦ░чЪДцЬАф╜│цибхЮЛх╖▓ф┐ЭхнШ (щкМшпБцНЯхд▒: {val_loss:.4f})")
            
            # хоЪцЬЯф┐ЭхнШцгАцЯечВ╣
            if (epoch + 1) % self.config.save_steps == 0:
                self.save_model(f"checkpoint_epoch_{epoch}")
            
            # V4.0хКЯшГ╜я╝ЪшЗкцИСш┐нф╗г
            if self.config.enable_self_iteration and hasattr(self, 'self_iteration_engine'):
                try:
                    iteration_result = self.model.self_iterate()
                    print(f"ЁЯФД шЗкцИСш┐нф╗гхоМцИР: {iteration_result.iteration_id}")
                except Exception as e:
                    print(f"тЪая╕П шЗкцИСш┐нф╗гхд▒ш┤е: {e}")
            
            # цЙУхН░шонч╗Гч╗Яшоб
            print(f"ЁЯУК шонч╗ГцНЯхд▒: {train_loss:.4f}, щкМшпБцНЯхд▒: {val_loss:.4f}")
        
        # хБЬцнвClock(╧Д)цЧ╢х║ПшзжхПСхЩи
        if self.config.enable_clock_trigger and hasattr(self.model, 'stop_clock_trigger'):
            self.model.stop_clock_trigger()
        
        print("ЁЯОЙ хвЮх╝║чЙИшзгцИСхНПшоошонч╗ГхоМцИРя╝Б")
        return self.training_history
    
    def train_epoch(self, train_loader: DataLoader) -> float:
        """шонч╗Гф╕Аф╕кepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = 0
        
        progress_bar = tqdm(train_loader, desc="шонч╗Гф╕н")
        
        for batch_idx, batch in enumerate(progress_bar):
            # х░ЖцХ░цНочз╗хИ░шо╛хдЗ
            input_ids = batch['input_ids'].to(self.device)
            labels = batch['labels'].to(self.device)
            cognitive_labels = {k: v.to(self.device) for k, v in batch['cognitive_labels'].items()}
            
            # хЙНхРСф╝ацТн
            outputs = self.model(input_ids, return_cognitive_state=True)
            logits = outputs['logits']
            cognitive_states = outputs.get('cognitive_states', [])
            
            # шобчоЧцНЯхд▒
            loss_dict = self.criterion(
                logits, labels, cognitive_states, cognitive_labels
            )
            
            total_loss = loss_dict['total_loss']
            
            # хПНхРСф╝ацТн
            total_loss.backward()
            
            # цвпх║жч┤пчзп
            if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                self.optimizer.step()
                self.scheduler.step()
                self.optimizer.zero_grad()
                self.global_step += 1
            
            # цЫ┤цЦ░ш┐Ых║жцЭб
            progress_bar.set_postfix({
                'loss': f"{total_loss.item():.4f}",
                'lm_loss': f"{loss_dict['lm_loss'].item():.4f}",
                'cognitive_loss': f"{loss_dict['cognitive_loss'].item():.4f}"
            })
            
            num_batches += 1
            
            # шо░х╜ХшодчЯечК╢цАБ
            if cognitive_states:
                self.training_history['cognitive_states'].append(
                    cognitive_states[-1].to_dict()
                )
        
        return total_loss.item() / num_batches if num_batches > 0 else 0.0
    
    def validate(self, val_loader: DataLoader) -> float:
        """щкМшпБ"""
        self.model.eval()
        total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="щкМшпБф╕н"):
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['labels'].to(self.device)
                cognitive_labels = {k: v.to(self.device) for k, v in batch['cognitive_labels'].items()}
                
                outputs = self.model(input_ids, return_cognitive_state=True)
                logits = outputs['logits']
                cognitive_states = outputs.get('cognitive_states', [])
                
                loss_dict = self.criterion(
                    logits, labels, cognitive_states, cognitive_labels
                )
                
                total_loss += loss_dict['total_loss'].item()
                num_batches += 1
        
        return total_loss / num_batches if num_batches > 0 else 0.0
    
    def save_model(self, path: str):
        """ф┐ЭхнШцибхЮЛ"""
        os.makedirs('checkpoints', exist_ok=True)
        save_path = os.path.join('checkpoints', f"{path}.pt")
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'config': self.config,
            'global_step': self.global_step,
            'epoch': self.epoch,
            'best_val_loss': self.best_val_loss,
            'training_history': self.training_history
        }, save_path)
        
        print(f"ЁЯТ╛ цибхЮЛх╖▓ф┐ЭхнШхИ░: {save_path}")
    
    def load_model(self, path: str):
        """хКаш╜╜цибхЮЛ"""
        checkpoint = torch.load(path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.global_step = checkpoint.get('global_step', 0)
        self.epoch = checkpoint.get('epoch', 0)
        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        self.training_history = checkpoint.get('training_history', self.training_history)
        
        print(f"ЁЯУВ цибхЮЛх╖▓ф╗О {path} хКаш╜╜")


def test_enhanced_training_system():
    """ц╡ЛшпХхвЮх╝║чЙИшонч╗Гч│╗ч╗Я"""
    print("ЁЯза ц╡ЛшпХхвЮх╝║чЙИшзгцИСшонч╗Гч│╗ч╗Я...")
    
    # хИЫх╗║щЕНч╜о
    config = EnhancedTrainingConfig(
        vocab_size=50000,
        d_model=512,
        num_layers=4,
        num_heads=8,
        batch_size=2,
        max_steps=100,
        enable_clock_trigger=True,
        clock_interval=10
    )
    
    # хИЫх╗║цибхЮЛ
    model = create_enhanced_jiewo_cognitive_transformer({
        'vocab_size': config.vocab_size,
        'd_model': config.d_model,
        'num_layers': config.num_layers,
        'num_heads': config.num_heads,
        'enable_clock_trigger': config.enable_clock_trigger,
        'clock_interval': config.clock_interval
    })
    
    # хИЫх╗║tokenizerя╝Иф╜┐чФиGPT-2ф╜Ьф╕║чд║ф╛Ля╝Й
    try:
        tokenizer = AutoTokenizer.from_pretrained('gpt2')
        tokenizer.pad_token = tokenizer.eos_token
    except:
        # хжВцЮЬцЧац│Хф╕Лш╜╜я╝МхИЫх╗║чоАхНХчЪДtokenizer
        class SimpleTokenizer:
            def __init__(self):
                self.vocab_size = config.vocab_size
                self.pad_token_id = 0
                self.eos_token_id = 1
            
            def encode(self, text, **kwargs):
                # чоАхНХчЪДtokenization
                tokens = [hash(word) % self.vocab_size for word in text.split()]
                # чбоф┐Эш┐ФхЫЮцнгчбочЪДх╜вчК╢хТМхблхЕЕ
                if 'max_length' in kwargs:
                    max_length = kwargs['max_length']
                    if len(tokens) < max_length:
                        tokens.extend([self.pad_token_id] * (max_length - len(tokens)))
                    else:
                        tokens = tokens[:max_length]
                
                if 'return_tensors' in kwargs and kwargs['return_tensors'] == 'pt':
                    return torch.tensor([tokens])
                else:
                    return tokens
            
            def decode(self, tokens):
                return " ".join([str(t) for t in tokens])
        
        tokenizer = SimpleTokenizer()
    
    # хИЫх╗║шонч╗ГхЩи
    trainer = EnhancedJieWoTrainer(config, model, tokenizer)
    
    # ц╡ЛшпХшонч╗Г
    print("ЁЯФД х╝АхзЛц╡ЛшпХшонч╗Г...")
    try:
        history = trainer.train()
        print("тЬЕ хвЮх╝║чЙИшонч╗Гч│╗ч╗Яц╡ЛшпХцИРхКЯя╝Б")
        print(f"ЁЯУК шонч╗ГхОЖхП▓: {len(history['losses'])} ш╜о")
    except Exception as e:
        print(f"тЪая╕П шонч╗Гц╡ЛшпХхд▒ш┤е: {e}")
    
    print("ЁЯОЙ хвЮх╝║чЙИшзгцИСшонч╗Гч│╗ч╗Яц╡ЛшпХхоМцИРя╝Б")


if __name__ == "__main__":
    test_enhanced_training_system() 