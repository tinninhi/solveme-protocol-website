#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•è§£æˆ‘è®¤çŸ¥æ¶æ„
"""

import torch
import torch.nn as nn

print("ğŸ§  å¼€å§‹æµ‹è¯•è§£æˆ‘è®¤çŸ¥æ¶æ„...")

# åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„è§£æˆ‘è®¤çŸ¥Transformer
class SimpleJieWoCognitiveTransformer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.vocab_size = config.get('vocab_size', 50000)
        self.d_model = config.get('d_model', 768)
        self.num_heads = config.get('num_heads', 12)
        
        # ç®€åŒ–çš„åµŒå…¥å±‚
        self.embedding = nn.Embedding(self.vocab_size, self.d_model)
        
        # ç®€åŒ–çš„æ³¨æ„åŠ›å±‚
        self.attention = nn.MultiheadAttention(self.d_model, self.num_heads, batch_first=True)
        
        # è¾“å‡ºå±‚
        self.output = nn.Linear(self.d_model, self.vocab_size)
        
        # è®¤çŸ¥çŠ¶æ€ç¼“å­˜
        self.cognitive_state_cache = None
    
    def forward(self, input_ids: torch.Tensor, return_cognitive_state: bool = False):
        # ç®€åŒ–çš„å‰å‘ä¼ æ’­
        embeddings = self.embedding(input_ids)
        output, attention_weights = self.attention(embeddings, embeddings, embeddings)
        logits = self.output(output)
        
        # åˆ›å»ºç®€åŒ–çš„è®¤çŸ¥çŠ¶æ€
        batch_size = input_ids.shape[0]
        device = input_ids.device
        
        if self.cognitive_state_cache is None:
            self.cognitive_state_cache = {
                'self_awareness': torch.randn(batch_size, self.d_model, device=device),
                'desire_vector': torch.randn(batch_size, self.d_model, device=device),
                'ethic_constraints': torch.randn(batch_size, self.d_model, device=device),
                'execution_path': torch.randn(batch_size, self.d_model, device=device),
                'reflection_feedback': torch.randn(batch_size, self.d_model, device=device),
                'cognitive_confidence': 0.8,
                'evolution_step': 0
            }
        
        output_dict = {
            'logits': logits,
            'hidden_states': output
        }
        
        if return_cognitive_state:
            output_dict['cognitive_state'] = self.cognitive_state_cache
        
        return output_dict
    
    def get_cognitive_state(self):
        return self.cognitive_state_cache
    
    def analyze_cognitive_state(self):
        if self.cognitive_state_cache is None:
            return {"error": "No cognitive state available"}
        
        state = self.cognitive_state_cache
        
        # è®¡ç®—å„ç»´åº¦çš„å¼ºåº¦
        self_strength = torch.norm(state['self_awareness'], dim=-1).mean().item()
        desire_strength = torch.norm(state['desire_vector'], dim=-1).mean().item()
        ethic_strength = torch.norm(state['ethic_constraints'], dim=-1).mean().item()
        path_strength = torch.norm(state['execution_path'], dim=-1).mean().item()
        reflection_strength = torch.norm(state['reflection_feedback'], dim=-1).mean().item()
        
        return {
            "self_awareness_strength": self_strength,
            "desire_strength": desire_strength,
            "ethic_strength": ethic_strength,
            "path_strength": path_strength,
            "reflection_strength": reflection_strength,
            "cognitive_confidence": state['cognitive_confidence'],
            "evolution_step": state['evolution_step'],
            "overall_confidence": (self_strength + desire_strength + ethic_strength + path_strength + reflection_strength) / 5
        }

def count_parameters(model: nn.Module):
    """è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    return {
        'total_parameters': total_params,
        'trainable_parameters': trainable_params,
        'non_trainable_parameters': total_params - trainable_params
    }

def test_jiewo_cognitive_architecture():
    """æµ‹è¯•è§£æˆ‘è®¤çŸ¥æ¶æ„"""
    print("ğŸ§  æµ‹è¯•è§£æˆ‘è®¤çŸ¥æ¶æ„...")
    
    # æ¨¡å‹é…ç½®
    config = {
        'vocab_size': 50000,
        'd_model': 512,
        'num_layers': 4,
        'num_heads': 8,
        'd_ff': 2048,
        'max_seq_length': 1024,
        'dropout': 0.1
    }
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleJieWoCognitiveTransformer(config)
    
    # è®¡ç®—å‚æ•°æ•°é‡
    param_counts = count_parameters(model)
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°: {param_counts['total_parameters']:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {param_counts['trainable_parameters']:,}")
    print(f"  ä¸å¯è®­ç»ƒå‚æ•°: {param_counts['non_trainable_parameters']:,}")
    
    # æµ‹è¯•è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    # æ¨¡æ‹Ÿè¾“å…¥
    batch_size = 2
    seq_len = 128
    input_ids = torch.randint(0, config['vocab_size'], (batch_size, seq_len)).to(device)
    
    # å‰å‘ä¼ æ’­æµ‹è¯•
    print("ğŸ”„ æµ‹è¯•å‰å‘ä¼ æ’­...")
    outputs = model(input_ids, return_cognitive_state=True)
    
    print(f"ğŸ“ˆ è¾“å‡ºå½¢çŠ¶:")
    print(f"  logits: {outputs['logits'].shape}")
    print(f"  hidden_states: {outputs['hidden_states'].shape}")
    
    # è®¤çŸ¥çŠ¶æ€åˆ†æ
    print("ğŸ§  æµ‹è¯•è®¤çŸ¥çŠ¶æ€åˆ†æ...")
    analysis = model.analyze_cognitive_state()
    
    print(f"ğŸ§  è®¤çŸ¥çŠ¶æ€åˆ†æ:")
    for key, value in analysis.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.4f}")
        else:
            print(f"  {key}: {value}")
    
    print("ğŸ‰ è§£æˆ‘è®¤çŸ¥æ¶æ„æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    test_jiewo_cognitive_architecture() 