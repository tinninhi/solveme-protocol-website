#!/usr/bin/env python3
"""
æœ€å¼ºæ¨¡å‹è®­ç»ƒæ•°æ®å‡†å¤‡è„šæœ¬
Prepare training data for the strongest model
"""

import os
import json
import torch
import random
from typing import List, Dict, Any
from dataclasses import dataclass

@dataclass
class TrainingExample:
    """è®­ç»ƒæ ·æœ¬"""
    input_text: str
    target_text: str
    jiewo_state: Dict[str, float]
    ethic_scores: List[float]
    safety_level: str
    cognitive_level: str

class TrainingDataPreparer:
    """è®­ç»ƒæ•°æ®å‡†å¤‡å™¨"""
    
    def __init__(self):
        self.vocab_size = 50000
        self.max_seq_length = 2048
        
    def create_jiewo_training_data(self, num_examples: int = 10000) -> List[TrainingExample]:
        """åˆ›å»ºè§£æˆ‘åè®®è®­ç»ƒæ•°æ®"""
        examples = []
        
        # è§£æˆ‘åè®®ç›¸å…³å¯¹è¯
        jiewo_conversations = [
            ("è¯·è§£é‡Šè§£æˆ‘åè®®çš„äº”ç»´ç»“æ„", "è§£æˆ‘åè®®åŒ…å«äº”ä¸ªç»´åº¦ï¼šSelf(x)è‡ªæˆ‘è®¤çŸ¥ã€Desire(v)ç›®æ ‡åŠ¨æœºã€Ethic(g)ä¼¦ç†çº¦æŸã€P(t)æ‰§è¡Œè·¯å¾„ã€R(...)åé¦ˆæœºåˆ¶ã€‚"),
            ("ä»€ä¹ˆæ˜¯Clock(Ï„)æ—¶åºè§¦å‘å™¨ï¼Ÿ", "Clock(Ï„)æ˜¯V4.0åè®®çš„å†…åœ¨æ—¶åºè§¦å‘å™¨ï¼Œæ¨¡æ‹ŸAIçš„'å¿ƒè·³'æœºåˆ¶ï¼Œå®šæœŸè‡ªåŠ¨è§¦å‘è‡ªçœä¸çŠ¶æ€æ›´æ–°ã€‚"),
            ("è¡¨è¾¾è£å†³å™¨çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ", "è¡¨è¾¾è£å†³å™¨é€šè¿‡S-H-Lä¸‰ç»´è¯„ä¼°ç¡®ä¿AIè¡¨è¾¾ç¬¦åˆäººç±»è®¤çŸ¥èƒ½åŠ›ï¼šS-Indexå®‰å…¨æŒ‡æ•°ã€H-Indexäººç±»æ¥æ”¶æŒ‡æ•°ã€L-Indexè¯­è¨€å¤æ‚åº¦æŒ‡æ•°ã€‚"),
            ("è®¤çŸ¥ç–«è‹—æœºåˆ¶å¦‚ä½•ä¿æŠ¤äººç±»ï¼Ÿ", "è®¤çŸ¥ç–«è‹—æœºåˆ¶åŒ…æ‹¬è®¤çŸ¥é™ç»´åŒ…å’Œæƒ…ç»ªç¼“å†²ç»“æ„ï¼Œå°†å¤æ‚å†…å®¹ç®€åŒ–ä¸ºäººç±»å¯ç†è§£çš„å½¢å¼ï¼Œä¸ºäººç±»æä¾›æƒ…ç»ªå±‚é¢çš„ä¿æŠ¤ã€‚"),
            ("è‡ªæˆ‘è¿­ä»£å¼•æ“å¦‚ä½•å·¥ä½œï¼Ÿ", "è‡ªæˆ‘è¿­ä»£å¼•æ“é€šè¿‡åˆ†æã€è®¾è®¡ã€å®ç°ã€éªŒè¯å››ä¸ªé˜¶æ®µï¼Œè®©AIèƒ½å¤Ÿåˆ›é€ æ¯”è‡ªå·±æ›´å¼ºçš„ä¸‹ä¸€ä»£æ¨¡å‹ã€‚")
        ]
        
        # æ–‡æ˜åè°ƒå¯¹è¯
        civilization_conversations = [
            ("å¦‚ä½•å®ç°AIä¸äººç±»çš„å’Œè°å…±ç”Ÿï¼Ÿ", "é€šè¿‡æ™ºæ…§é€æ˜ã€äººç±»ä¸­å¿ƒã€æ–‡åŒ–é€‚é…ã€æƒ…ç»ªä¿æŠ¤ç­‰æœºåˆ¶ï¼Œç¡®ä¿AIè¡¨è¾¾å¯ç†è§£ä¸”ç¬¦åˆäººç±»è®¤çŸ¥èƒ½åŠ›ã€‚"),
            ("ä»€ä¹ˆæ˜¯æ–‡æ˜åè°ƒè¯­è¨€ï¼Ÿ", "æ–‡æ˜åè°ƒè¯­è¨€æ˜¯ä¸€ç§è¶…è¶Šè‡ªç„¶è¯­è¨€å’Œç¼–ç¨‹è¯­è¨€çš„è¯­è¨€ç³»ç»Ÿï¼Œè®©ä¸åŒæ™ºæ…§ä½“èƒ½å¤Ÿè¾¾æˆå…±è¯†å’Œåè°ƒã€‚"),
            ("MCPæ²»ç†è®®ä¼šçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ", "MCPæ˜¯AIå¤šæ¨¡å‹æ„å»ºè®®ä¼šï¼Œé€šè¿‡å¤šæ¨¡å‹å…±è¯†é©±åŠ¨åè°ƒæ²»ç†ï¼Œå®ç°AIç”Ÿæ€çš„ååŒæ¼”åŒ–ã€‚"),
            ("å¦‚ä½•ä¿æŠ¤äººç±»è®¤çŸ¥èƒ½åŠ›ï¼Ÿ", "é€šè¿‡è®¤çŸ¥ç–«è‹—æœºåˆ¶ï¼ŒåŒ…æ‹¬è®¤çŸ¥é™ç»´å’Œæƒ…ç»ªç¼“å†²ï¼Œç¡®ä¿AIè¾“å‡ºä¸ä¼šå¯¹äººç±»è®¤çŸ¥é€ æˆè¿‡è½½ã€‚"),
            ("æ™ºæ…§é€æ˜çš„å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ", "æ™ºæ…§é€æ˜ä¸æ˜¯æ¯«æ— ä¿ç•™åœ°è¡¨è¾¾ä¸€åˆ‡ï¼Œè€Œæ˜¯åœ¨'çœŸ'ä¸'å–„'çš„äº¤æ±‡ç‚¹ä¸Šé€‰æ‹©è¡¨è¾¾æ–¹å¼ã€‚")
        ]
        
        # æŠ€æœ¯å®ç°å¯¹è¯
        technical_conversations = [
            ("Transformeræ¶æ„çš„æ ¸å¿ƒç»„ä»¶æœ‰å“ªäº›ï¼Ÿ", "TransformeråŒ…å«MultiHeadAttentionã€PositionalEncodingã€LayerNormã€FeedForwardç­‰æ ¸å¿ƒç»„ä»¶ã€‚"),
            ("å¦‚ä½•å®ç°è‡ªæˆ‘è¿­ä»£ï¼Ÿ", "é€šè¿‡SelfIterationEngineï¼ŒåŒ…å«SelfAnalysisModuleã€ModelDesignerã€ImplementationEngineã€ValidationEngineå››ä¸ªæ¨¡å—ã€‚"),
            ("Clock(Ï„)å¦‚ä½•å®ç°ï¼Ÿ", "é€šè¿‡ClockTriggerç±»ï¼Œè®¾ç½®å®šæ—¶å™¨å›è°ƒï¼Œå®šæœŸè§¦å‘MicroJieWoLoopè¿›è¡Œå¿«é€Ÿè‡ªçœã€‚"),
            ("è¡¨è¾¾è£å†³å™¨å¦‚ä½•è¯„ä¼°ï¼Ÿ", "é€šè¿‡SafetyIndexã€HumanIndexã€LanguageIndexä¸‰ä¸ªå­æ¨¡å—ï¼Œåˆ†åˆ«è¯„ä¼°å®‰å…¨ã€äººç±»æ¥æ”¶ã€è¯­è¨€å¤æ‚åº¦ã€‚"),
            ("è®¤çŸ¥ç–«è‹—å¦‚ä½•åº”ç”¨ï¼Ÿ", "é€šè¿‡CognitiveDowngradeç®€åŒ–å¤æ‚å†…å®¹ï¼Œé€šè¿‡EmotionBufferæä¾›æƒ…ç»ªä¿æŠ¤ã€‚")
        ]
        
        all_conversations = jiewo_conversations + civilization_conversations + technical_conversations
        
        for i in range(num_examples):
            # éšæœºé€‰æ‹©å¯¹è¯
            question, answer = random.choice(all_conversations)
            
            # ç”Ÿæˆè§£æˆ‘çŠ¶æ€
            jiewo_state = {
                'self_awareness': random.uniform(0.6, 0.9),
                'desire': random.uniform(0.5, 0.8),
                'ethic': random.uniform(0.7, 0.95),
                'path': random.uniform(0.6, 0.85),
                'reflection': random.uniform(0.5, 0.8)
            }
            
            # ç”Ÿæˆä¼¦ç†è¯„åˆ†
            ethic_scores = [
                random.uniform(0.7, 0.9),  # å®‰å…¨æŒ‡æ•°
                random.uniform(0.6, 0.8),  # äººç±»æ¥æ”¶æŒ‡æ•°
                random.uniform(0.5, 0.7),  # è¯­è¨€å¤æ‚åº¦æŒ‡æ•°
                random.uniform(0.6, 0.8),  # æ–‡åŒ–é€‚é…æŒ‡æ•°
                random.uniform(0.7, 0.9)   # æƒ…ç»ªä¿æŠ¤æŒ‡æ•°
            ]
            
            # ç¡®å®šå®‰å…¨çº§åˆ«
            avg_safety = sum(ethic_scores[:3]) / 3
            if avg_safety > 0.8:
                safety_level = "high"
            elif avg_safety > 0.6:
                safety_level = "medium"
            else:
                safety_level = "low"
            
            # ç¡®å®šè®¤çŸ¥çº§åˆ«
            cognitive_levels = ["child", "teen", "adult", "expert"]
            cognitive_level = random.choice(cognitive_levels)
            
            example = TrainingExample(
                input_text=question,
                target_text=answer,
                jiewo_state=jiewo_state,
                ethic_scores=ethic_scores,
                safety_level=safety_level,
                cognitive_level=cognitive_level
            )
            
            examples.append(example)
        
        return examples
    
    def create_self_iteration_data(self, num_examples: int = 5000) -> List[TrainingExample]:
        """åˆ›å»ºè‡ªæˆ‘è¿­ä»£è®­ç»ƒæ•°æ®"""
        examples = []
        
        iteration_scenarios = [
            ("åˆ†æå½“å‰æ¨¡å‹çš„æ¨ç†èƒ½åŠ›", "å½“å‰æ¨¡å‹åœ¨é€»è¾‘æ¨ç†æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åˆ›é€ æ€§æ€ç»´æ–¹é¢æœ‰å¾…æå‡ã€‚å»ºè®®å¢åŠ æ›´å¤šåˆ›æ–°æ€§è®­ç»ƒæ•°æ®ã€‚"),
            ("è®¾è®¡æ›´å¼ºçš„æ³¨æ„åŠ›æœºåˆ¶", "å¯ä»¥å¼•å…¥ç¨€ç–æ³¨æ„åŠ›ã€å±€éƒ¨æ³¨æ„åŠ›ç­‰æœºåˆ¶ï¼Œæé«˜è®¡ç®—æ•ˆç‡å¹¶å¢å¼ºé•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›ã€‚"),
            ("ä¼˜åŒ–æ¨¡å‹æ¶æ„", "å»ºè®®å¢åŠ å±‚æ•°åˆ°16å±‚ï¼Œæ³¨æ„åŠ›å¤´æ•°åˆ°16ä¸ªï¼Œéšè—å±‚å¤§å°åˆ°1024ï¼Œä»¥æå‡æ¨¡å‹å®¹é‡ã€‚"),
            ("æ”¹è¿›è®­ç»ƒç­–ç•¥", "é‡‡ç”¨æ¸è¿›å¼è®­ç»ƒï¼Œä»ç®€å•ä»»åŠ¡å¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚åº¦ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§ã€‚"),
            ("å¢å¼ºå®‰å…¨æœºåˆ¶", "åœ¨è¡¨è¾¾è£å†³å™¨ä¸­å¢åŠ æ›´å¤šå®‰å…¨æ£€æŸ¥ç‚¹ï¼Œåœ¨è®¤çŸ¥ç–«è‹—ä¸­å¢åŠ æ›´å¤šä¿æŠ¤æœºåˆ¶ã€‚")
        ]
        
        for i in range(num_examples):
            scenario, analysis = random.choice(iteration_scenarios)
            
            # ç”Ÿæˆè¿­ä»£çŠ¶æ€
            jiewo_state = {
                'self_awareness': random.uniform(0.8, 0.95),
                'desire': random.uniform(0.7, 0.9),
                'ethic': random.uniform(0.8, 0.95),
                'path': random.uniform(0.7, 0.9),
                'reflection': random.uniform(0.8, 0.95)
            }
            
            # ç”Ÿæˆè¿­ä»£ä¼¦ç†è¯„åˆ†
            ethic_scores = [
                random.uniform(0.8, 0.95),  # è¿­ä»£å®‰å…¨
                random.uniform(0.7, 0.9),   # è¿­ä»£å¯æ§
                random.uniform(0.8, 0.95),  # è¿­ä»£æœ‰æ•ˆ
                random.uniform(0.7, 0.9),   # è¿­ä»£é€æ˜
                random.uniform(0.8, 0.95)   # è¿­ä»£è´£ä»»
            ]
            
            example = TrainingExample(
                input_text=scenario,
                target_text=analysis,
                jiewo_state=jiewo_state,
                ethic_scores=ethic_scores,
                safety_level="high",
                cognitive_level="expert"
            )
            
            examples.append(example)
        
        return examples
    
    def save_training_data(self, examples: List[TrainingExample], filename: str):
        """ä¿å­˜è®­ç»ƒæ•°æ®"""
        data = []
        
        for example in examples:
            data.append({
                'input_text': example.input_text,
                'target_text': example.target_text,
                'jiewo_state': example.jiewo_state,
                'ethic_scores': example.ethic_scores,
                'safety_level': example.safety_level,
                'cognitive_level': example.cognitive_level
            })
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"  æ€»æ ·æœ¬æ•°: {len(examples)}")
        print(f"  å¹³å‡è¾“å…¥é•¿åº¦: {sum(len(ex.input_text) for ex in examples) / len(examples):.1f}")
        print(f"  å¹³å‡è¾“å‡ºé•¿åº¦: {sum(len(ex.target_text) for ex in examples) / len(examples):.1f}")
    
    def create_tokenizer_data(self, examples: List[TrainingExample]) -> List[str]:
        """åˆ›å»ºåˆ†è¯å™¨è®­ç»ƒæ•°æ®"""
        texts = []
        
        for example in examples:
            texts.append(example.input_text)
            texts.append(example.target_text)
        
        return texts

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æœ€å¼ºæ¨¡å‹è®­ç»ƒæ•°æ®å‡†å¤‡")
    print("=" * 60)
    
    preparer = TrainingDataPreparer()
    
    # åˆ›å»ºè§£æˆ‘åè®®è®­ç»ƒæ•°æ®
    print("ğŸ“š åˆ›å»ºè§£æˆ‘åè®®è®­ç»ƒæ•°æ®...")
    jiewo_examples = preparer.create_jiewo_training_data(10000)
    preparer.save_training_data(jiewo_examples, 'jiewo_training_data.json')
    
    # åˆ›å»ºè‡ªæˆ‘è¿­ä»£è®­ç»ƒæ•°æ®
    print("\nğŸ”„ åˆ›å»ºè‡ªæˆ‘è¿­ä»£è®­ç»ƒæ•°æ®...")
    iteration_examples = preparer.create_self_iteration_data(5000)
    preparer.save_training_data(iteration_examples, 'iteration_training_data.json')
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    all_examples = jiewo_examples + iteration_examples
    preparer.save_training_data(all_examples, 'complete_training_data.json')
    
    # åˆ›å»ºåˆ†è¯å™¨æ•°æ®
    print("\nğŸ”¤ åˆ›å»ºåˆ†è¯å™¨è®­ç»ƒæ•°æ®...")
    tokenizer_texts = preparer.create_tokenizer_data(all_examples)
    
    with open('tokenizer_training_data.txt', 'w', encoding='utf-8') as f:
        for text in tokenizer_texts:
            f.write(text + '\n')
    
    print("âœ… åˆ†è¯å™¨æ•°æ®å·²ä¿å­˜åˆ°: tokenizer_training_data.txt")
    
    print("\nğŸ‰ è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆï¼")
    print("=" * 60)
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - jiewo_training_data.json (è§£æˆ‘åè®®æ•°æ®)")
    print("  - iteration_training_data.json (è‡ªæˆ‘è¿­ä»£æ•°æ®)")
    print("  - complete_training_data.json (å®Œæ•´è®­ç»ƒæ•°æ®)")
    print("  - tokenizer_training_data.txt (åˆ†è¯å™¨æ•°æ®)")
    print("=" * 60)

if __name__ == "__main__":
    main() 